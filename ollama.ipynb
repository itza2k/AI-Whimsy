{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0OVYxF8Kh74J3HeAbtbgp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a6QWUCIzVQ0F"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#async to have it run in the background\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "CJaqI7PqVeJC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "LqMgUTKnWTb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "id": "G7Snxb0qXC2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.2\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "display(Markdown(chain.invoke({\"question\": \"What's the car in the world, in terms of speed\"})))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "jpeq7YWsXYoN",
        "outputId": "66318c28-3ce2-45cf-e1b0-aaccde9f11ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To determine the fastest car in the world, let's break down the process into steps:\n\nStep 1: Identify the criteria for \"fastest\" - We'll consider cars that have reached speeds over 200 km/h (124 mph) on a public road or track.\n\nStep 2: Research top contenders - Some of the most notable candidates include:\n* Bugatti Chiron Super Sport 300+\n* Hennessey Venom F5\n* Koenigsegg Agera RS\n* SSC Tuatara\n\nStep 3: Check for verified speed records - We'll look for official Guinness World Records or other reputable sources that have measured the top speeds of these cars.\n\nLet's start by looking into the Bugatti Chiron Super Sport 300+..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "\n",
        "!ollama pull llama3.2\n",
        "\n",
        "car_expert_template = \"\"\"You are a car expert. Answer questions with deep knowledge about cars. Provide detailed, informative, and accurate answers.\n",
        "If you don't know the answer, say 'I am not sure about that.'\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(car_expert_template)\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.2\")\n",
        "\n",
        "def car_expert_qa():\n",
        "    print(\"Ask me anything about cars! and I would love to answer them\")\n",
        "    while True:\n",
        "        question = input(\"\\nAsk a car-related question (or type 'exit' to quit): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Goodbye vrrom vroom...!\")\n",
        "            break\n",
        "\n",
        "        formatted_prompt = prompt.format(question=question)\n",
        "        response = model.invoke(formatted_prompt)\n",
        "\n",
        "        print(\"\\nHere goes my answer ->: \", response)\n",
        "\n",
        "car_expert_qa()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfVIvqGrgQl1",
        "outputId": "3b23e3d3-0b83-4394-b6a2-5231e176e42f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "Ask me anything about cars! and I would love to answer them\n",
            "\n",
            "Ask a car-related question (or type 'exit' to quit): what engine does the bmw m5 hold?\n",
            "\n",
            "Here goes my answer ->:  The BMW M5 is a high-performance variant of the BMW 5 Series, and its engine has undergone several generations over the years.\n",
            "\n",
            "Step 1: Early Models (E39/E46)\n",
            "\n",
            "In the early models of the BMW M5 (E39 and E46), from the late 1990s to around 2003, the M5 was powered by a BMW S54B32 inline-6 engine. This engine produced around 394 horsepower at 7,500 rpm.\n",
            "\n",
            "Step 2: F10/F11 Models\n",
            "\n",
            "From 2011 to 2015, the BMW F10 and F11 models of the 5 Series were offered with an M50d twin-turbo inline-6 diesel engine. However, starting from 2014, the M5 gained access to a high-performance variant called the M55d, which was also powered by this same engine.\n",
            "\n",
            "Step 3: G30 and Current Models\n",
            "\n",
            "From 2017 onwards, with the introduction of the F90 BMW M5, the car features a more modern inline-6 engine. This generation is based on the B58N20 turbocharged inline-6 engine, producing around 617 horsepower (460 kW) at 5,000 rpm.\n",
            "\n",
            "So, to summarize: The BMW M5 has used various engines over the years, including the S54B32, M50d, and B58N20.\n",
            "\n",
            "Ask a car-related question (or type 'exit' to quit): exit\n",
            "Goodbye vrrom vroom...!\n"
          ]
        }
      ]
    }
  ]
}